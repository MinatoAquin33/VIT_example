{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283133e0-b90c-4aeb-ae42-09ae1d813dda",
   "metadata": {},
   "source": [
    "# Êï∞ÊçÆÂØºÂÖ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239229a2-576d-4def-a075-5c0249d1b1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26462c8ea974a5f9fe4f465e7ec88de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886b9787313d479f91e7380cc2e26b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(path=\"imagefolder\", data_dir=\"E:\\workdata\\hfdataset\") #Ë∑ØÂæÑ‰∏çËÉΩÊúâ‰∏≠Êñá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bd4f5f-6bfc-4c9e-af59-c6c62a62232a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 5380\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 312\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e026a12-3966-46aa-b467-ec63df4dbdcf",
   "metadata": {},
   "source": [
    "# Âà©Áî®VITËÆ≠ÁªÉÊ®°Âûã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931c78e-5e2b-43dd-9f46-f9fa8050dd3f",
   "metadata": {},
   "source": [
    "### Â∞ÜÂõæÂÉèËΩ¨Êç¢‰∏∫Âº†Èáè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1a69de-a4f9-4e1a-a122-54a119e86910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'label', 'pixel_values'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# Âä†ËΩΩÂõæÂÉèÂ§ÑÁêÜÂô®\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-hybrid-base-bit-384\")\n",
    "\n",
    "def transforms(examples):\n",
    "    images = [img.convert(\"RGB\").resize((384, 384)) for img in examples[\"image\"]]  # ÂõæÁâáË¢´ËΩ¨Êç¢‰∏∫RGBÈÄöÈÅìÔºåÂêåÊó∂Áº©ÊîæËá≥384*384\n",
    "    \n",
    "    examples[\"pixel_values\"] = image_processor(images, return_tensors=\"pt\")[\"pixel_values\"] # ‰ΩøÁî®image_processorÂ§ÑÁêÜÂõæÂÉèÔºåÁîüÊàêpixel_valuesÔºàÂº†ÈáèÂõæÂÉèÔºâ\n",
    "    return examples\n",
    "\n",
    "dataset.set_transform(transforms)\n",
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1f8bb-e578-4d15-8570-d0ae21e5986e",
   "metadata": {},
   "source": [
    "### ÂÆö‰πâÊï∞ÊçÆÊï¥ÁêÜÂô®ÔºåÁî®‰∫éÂØπÂõæÂÉèÂàÜÊâπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3339da-57da-46af-8be9-e74be461d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch): # batchÂ∫îÂåÖÂê´pixel_valuesÂíålabels\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]), # ËøîÂõûÂ†ÜÂè†ÁöÑÂõæÂÉèÂº†Èáè\n",
    "        'labels': torch.tensor([x['labels'] for x in batch]) # ËøîÂõûÂ†ÜÂè†ÁöÑÊ†áÁ≠æÂº†Èáè\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4de7c5-0f51-4c31-a37d-84bf878cc51c",
   "metadata": {},
   "source": [
    "### ÂÆö‰πâËØÑ‰ª∑Âô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d76eb31-1049-465e-804d-ca46ab714dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\w1586\\AppData\\Local\\Temp\\ipykernel_35944\\3223945255.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n",
      "C:\\Users\\w1586\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\w1586\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\w1586\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\w1586\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # ËÆ°ÁÆóÈ¢ÑÊµãÁªìÊûú\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "\n",
    "    # ËÆ°ÁÆóÂêÑ‰∏™ÊåáÊ†á\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=p.label_ids)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=p.label_ids, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=p.label_ids, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=p.label_ids, average='weighted')\n",
    "\n",
    "    # ËæìÂá∫ÁªìÊûú\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3be24b-132e-4b79-a3bb-3c9a476fd1c1",
   "metadata": {},
   "source": [
    "### ‰øÆÊîπlabel‰∏∫labels‰ª•Á¨¶ÂêàVITË¶ÅÊ±Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a272773-645f-4ac2-aab2-2dd1397ea5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].rename_column('label', 'labels') # Â∞Ü 'label' ÁâπÂæÅÂêçÊîπ‰∏∫ 'labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678f6c5-53e9-421c-87d0-6a30fefd89fa",
   "metadata": {},
   "source": [
    "### Âä†ËΩΩÂàùÂßãÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0560d0d5-2ec4-4445-82c7-75a3ecd388f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type vit-hybrid to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at google/vit-hybrid-base-bit-384 were not used when initializing ViTForImageClassification: ['vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.embedder.norm.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.downsample.norm.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.downsample.conv.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.embedder.convolution.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.downsample.norm.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.downsample.norm.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.downsample.conv.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.downsample.norm.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.8.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.0.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.embedder.norm.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.3.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.3.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.conv2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.downsample.norm.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.2.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.downsample.norm.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.conv1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.7.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.5.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.4.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.1.norm2.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.downsample.conv.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.6.norm1.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.1.norm1.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.2.norm3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.1.layers.1.norm3.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.2.conv3.weight', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.0.layers.0.norm2.bias', 'vit.embeddings.patch_embeddings.backbone.bit.encoder.stages.2.layers.0.norm3.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-hybrid-base-bit-384 and are newly initialized because the shapes did not match:\n",
      "- vit.embeddings.position_embeddings: found shape torch.Size([1, 577, 768]) in the checkpoint and torch.Size([1, 147457, 768]) in the model instantiated\n",
      "- vit.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 1024, 1, 1]) in the checkpoint and torch.Size([768, 3, 1, 1]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model_name = 'google/vit-hybrid-base-bit-384' # ÊåáÂÆöVITÊ®°Âûã\n",
    "\n",
    "labels = dataset['train'].features['labels'].names # ÊèêÂèñÊ†áÁ≠æ‰ø°ÊÅØ\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels), # ÊåáÂÆöÊ®°ÂûãËæìÂá∫Â±ÇÁöÑÂ§ßÂ∞è(Á±ªÂà´Êï∞)\n",
    "    id2label={str(i): c for i, c in enumerate(labels)}, #Âª∫Á´ãÊ†áÁ≠æÂíåÁ±ªÂà´Á¥¢ÂºïÁöÑÊò†Â∞ÑÂÖ≥Á≥ª\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea9741-43fd-4b75-adf6-07ca4fb70ea5",
   "metadata": {},
   "source": [
    "### ËÆæÂÆöËÆ≠ÁªÉÂèÇÊï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb7f3d5-fc9d-4f0e-a2a9-90a24255eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-Hybird-covid\", # Ê®°Âûã‰øùÂ≠òÁöÑÁõÆÂΩï\n",
    "  per_device_train_batch_size=512, # Ë∂äÂ§ßÊ®°ÂûãË∂äÁ®≥ÂÆö‰ΩÜÈúÄË¶ÅÊ∂àËÄóÊõ¥Â§ßÊòæÂ≠ò\n",
    "  evaluation_strategy=\"steps\", #Ê®°ÂûãËØÑ‰º∞Á≠ñÁï•ÔºåstepsË°®Á§∫ÊØèËøá‰∏ÄÂÆöÊ¨°Êï∞ËØÑ‰º∞‰∏ÄÊ¨°Ê®°Âûã\n",
    "  num_train_epochs=100, #ËÆ≠ÁªÉËΩÆÊï∞\n",
    "  fp16=True, #ÂçäÁ≤æÂ∫¶ÊµÆÁÇπÔºåÂèØÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®ÔºåÈúÄË¶ÅËÆæÂ§áÊîØÊåÅ\n",
    "  save_steps=100, #Â§öÂ∞ëÊ≠•‰øùÂ≠ò‰∏ÄÊ¨°Ê®°Âûã\n",
    "  eval_steps=100, #Â§öÂ∞ëÊ≠•ËØÑ‰º∞‰∏ÄÊ¨°Ê®°Âûã\n",
    "  logging_steps=3, #Â§öÂ∞ëÊ≠•ËØÑ‰º∞‰∏ÄÊ¨°Êó•Âøó\n",
    "  learning_rate=5e-5, #Â≠¶‰π†Áéá Ë∂ä‰ΩéË∂äÂÆπÊòìËøáÊãüÂêàÔºåË∂äÈ´òÊî∂ÊïõË∂äÊÖ¢\n",
    "  save_total_limit=3, #‰øùÂ≠òÁöÑÊ®°ÂûãÊÄªÊï∞\n",
    "  remove_unused_columns=False, #Âà†Èô§Êú™‰ΩøÁî®ÂàóÔºåÂáèÂ∞ëÂÜÖÂ≠òÊ∂àËÄó\n",
    "  push_to_hub=False, #ÊòØÂê¶Â∞ÜÊ®°ÂûãÂèëÂ∏ÉÂà∞hfÁ§æÂå∫\n",
    "  report_to='tensorboard', #Êó•ÂøóÁöÑÊä•ÂëäÂú∞\n",
    "  load_best_model_at_end=True, #ÊòØÂê¶Ëá™Âä®Âä†ËΩΩÊúÄ‰ºòÊ®°Âûã\n",
    "  ignore_data_skip=True #‰ªéÊñ≠ÁÇπÁªßÁª≠ËÆ≠ÁªÉÊ®°Âûã\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9293ca-5d39-4e7a-8b22-5baa0ebc8f0a",
   "metadata": {},
   "source": [
    "##### Êõ¥Â§öÂèÇÊï∞Ëß£Èáä https://zhuanlan.zhihu.com/p/363670628"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6501a03-0825-4ed0-9bc7-0315deecbd6e",
   "metadata": {},
   "source": [
    "### Â∞ÜÂèÇÊï∞‰º†ÂÖ•ËÆ≠ÁªÉÂô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05477ba-97dd-4efc-b97f-0040a099cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1f13f-f614-49b2-b998-2a870da62952",
   "metadata": {},
   "source": [
    "### ËÆ≠ÁªÉÂºÄÂßã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0f7403-1585-4725-be60-c18ae8ea0e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\w1586\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 108.00 GiB. GPU 0 has a total capacity of 8.00 GiB of which 4.87 GiB is free. Of the allocated memory 2.00 GiB is allocated by PyTorch, and 39.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1905\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1906\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1907\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1908\u001b[0m ):\n\u001b[0;32m   1909\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2645\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2648\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2676\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2677\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:787\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    785\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 787\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    798\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:573\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[0;32m    571\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m--> 573\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    578\u001b[0m     embedding_output,\n\u001b[0;32m    579\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:117\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    112\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    113\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    114\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    116\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 117\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         seq_length \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:175\u001b[0m, in \u001b[0;36mViTPatchEmbeddings.forward\u001b[1;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m         )\n\u001b[1;32m--> 175\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 108.00 GiB. GPU 0 has a total capacity of 8.00 GiB of which 4.87 GiB is free. Of the allocated memory 2.00 GiB is allocated by PyTorch, and 39.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65518ad-17fd-4f79-9110-e7a3f7dc4db4",
   "metadata": {},
   "source": [
    "# ÊÄßËÉΩÈ™åËØÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88e00d-3c6a-4f3b-9e7f-c72601fd303b",
   "metadata": {},
   "source": [
    "## Âü∫ÂáÜÊµãËØï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a1515bd-c084-4987-91da-371eb0ffe967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =      100.0\n",
      "  eval_accuracy           =     0.8397\n",
      "  eval_f1                 =     0.8355\n",
      "  eval_loss               =     0.3775\n",
      "  eval_precision          =     0.8557\n",
      "  eval_recall             =     0.8278\n",
      "  eval_runtime            = 0:00:15.93\n",
      "  eval_samples_per_second =     19.584\n",
      "  eval_steps_per_second   =      2.448\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(dataset['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a481286-1c6a-454f-a5f4-d7d82ba0fa21",
   "metadata": {},
   "source": [
    "## Â§ñÈÉ®È™åËØÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0f3a9-8672-4a7b-82b5-dc95d93c45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "Outerdataset = load_dataset(path=\"imagefolder\", data_dir=\"E:/jupyter/VIT_example/dataset/Outerdataset\") #Ë∑ØÂæÑ‰∏çËÉΩÊúâ‰∏≠Êñá\n",
    "Outerdataset.set_transform(transforms)\n",
    "Outerdataset['test'][0].keys()\n",
    "for split in Outerdataset:\n",
    "    Outerdataset[split] = Outerdataset[split].rename_column('label', 'labels') # Â∞Ü 'label' ÁâπÂæÅÂêçÊîπ‰∏∫ 'labels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b1ee2-560e-4c0b-ab47-3444f93ed5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(Outerdataset[\"test\"])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6de539-1c35-4951-ab72-9f9d5ab4b9b2",
   "metadata": {},
   "source": [
    "## ÂØπÁâπÂÆöÂõæÁâáËøõË°åÈ¢ÑÊµã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac273302-30bf-4449-b7c5-d81e814a4379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1458x1303>,\n",
       " 'labels': 0,\n",
       " 'pixel_values': tensor([[[-0.6392, -0.6314, -0.6235,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          [-0.6078, -0.6000, -0.6000,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          [-0.5922, -0.5843, -0.5765,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          ...,\n",
       "          [-0.1216, -0.0980, -0.0824,  ..., -0.3647, -0.3804, -0.3882],\n",
       "          [-0.1137, -0.0980, -0.0902,  ..., -0.3725, -0.3804, -0.3961],\n",
       "          [-0.1137, -0.0980, -0.0824,  ..., -0.3647, -0.3804, -0.3961]],\n",
       " \n",
       "         [[-0.6392, -0.6314, -0.6235,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          [-0.6078, -0.6000, -0.6000,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          [-0.5922, -0.5843, -0.5765,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          ...,\n",
       "          [-0.1216, -0.0980, -0.0824,  ..., -0.3647, -0.3804, -0.3882],\n",
       "          [-0.1137, -0.0980, -0.0902,  ..., -0.3725, -0.3804, -0.3961],\n",
       "          [-0.1137, -0.0980, -0.0824,  ..., -0.3647, -0.3804, -0.3961]],\n",
       " \n",
       "         [[-0.6392, -0.6314, -0.6235,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          [-0.6078, -0.6000, -0.6000,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          [-0.5922, -0.5843, -0.5765,  ..., -0.9843, -0.9843, -0.9843],\n",
       "          ...,\n",
       "          [-0.1216, -0.0980, -0.0824,  ..., -0.3647, -0.3804, -0.3882],\n",
       "          [-0.1137, -0.0980, -0.0902,  ..., -0.3725, -0.3804, -0.3961],\n",
       "          [-0.1137, -0.0980, -0.0824,  ..., -0.3647, -0.3804, -0.3961]]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num=1\n",
    "image=dataset[\"test\"][num][\"image\"]\n",
    "dataset[\"test\"][num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "242ce90c-b56b-45b4-9a11-a4fdb97ff95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9786100387573242, 'label': 'Covid'},\n",
       " {'score': 0.018465569242835045, 'label': 'Normal'},\n",
       " {'score': 0.0024604233913123608, 'label': 'Lung Opacity'},\n",
       " {'score': 0.0004639296093955636, 'label': 'Viral Pneumonia'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model='./vit-Hybird-covid/' )\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3741e4-8bf3-4059-aa37-44562b094b6e",
   "metadata": {},
   "source": [
    "## ËøõË°åCovid‰∏éÈùûCovidÁöÑ‰∫åÂàÜÁ±ªÈ¢ÑÊµã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2155086-caff-45b7-928e-e954dcd6b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    # Â∞ÜÈ¢ÑÊµãÁªìÊûú‰∏≠ÁöÑÊâÄÊúâÈùû0ÔºàÈùûCovidÔºâÊ†áÁ≠æËΩ¨Êç¢‰∏∫1\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    binary_predictions = np.where(predictions == 0, 0, 1)\n",
    "\n",
    "    # Â∞ÜÁúüÂÆûÊ†áÁ≠æ‰∏≠ÁöÑÊâÄÊúâÈùû0ÔºàÈùûCovidÔºâÊ†áÁ≠æËΩ¨Êç¢‰∏∫1\n",
    "    binary_references = np.where(p.label_ids == 0, 0, 1)\n",
    "\n",
    "    # ËÆ°ÁÆó‰∫åÂàÜÁ±ªÁöÑÂêÑ‰∏™ÊåáÊ†á\n",
    "    accuracy = accuracy_metric.compute(predictions=binary_predictions, references=binary_references)['accuracy']\n",
    "    f1 = f1_metric.compute(predictions=binary_predictions, references=binary_references, average='binary')['f1']\n",
    "    precision = precision_metric.compute(predictions=binary_predictions, references=binary_references, average='binary')['precision']\n",
    "    recall = recall_metric.compute(predictions=binary_predictions, references=binary_references, average='binary')['recall']\n",
    "\n",
    "    # ËøîÂõûËÆ°ÁÆóÁöÑÊåáÊ†á\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88f035b5-a2d3-43ab-8829-ec6cb0fdbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-Hybird-covid\", # Ê®°Âûã‰øùÂ≠òÁöÑÁõÆÂΩï\n",
    "  per_device_train_batch_size=16, # Ë∂äÂ§ßÊ®°ÂûãË∂äÁ®≥ÂÆö‰ΩÜÈúÄË¶ÅÊ∂àËÄóÊõ¥Â§ßÊòæÂ≠ò\n",
    "  evaluation_strategy=\"steps\", #Ê®°ÂûãËØÑ‰º∞Á≠ñÁï•ÔºåstepsË°®Á§∫ÊØèËøá‰∏ÄÂÆöÊ¨°Êï∞ËØÑ‰º∞‰∏ÄÊ¨°Ê®°Âûã\n",
    "  num_train_epochs=100, #ËÆ≠ÁªÉËΩÆÊï∞\n",
    "  fp16=True, #ÂçäÁ≤æÂ∫¶ÊµÆÁÇπÔºåÂèØÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®ÔºåÈúÄË¶ÅËÆæÂ§áÊîØÊåÅ\n",
    "  save_steps=100, #Â§öÂ∞ëÊ≠•‰øùÂ≠ò‰∏ÄÊ¨°Ê®°Âûã\n",
    "  eval_steps=100, #Â§öÂ∞ëÊ≠•ËØÑ‰º∞‰∏ÄÊ¨°Ê®°Âûã\n",
    "  logging_steps=3, #Â§öÂ∞ëÊ≠•ËØÑ‰º∞‰∏ÄÊ¨°Êó•Âøó\n",
    "  learning_rate=2e-4, #Â≠¶‰π†Áéá Ë∂ä‰ΩéË∂äÂÆπÊòìËøáÊãüÂêàÔºåË∂äÈ´òÊî∂ÊïõË∂äÊÖ¢\n",
    "  save_total_limit=10, #‰øùÂ≠òÁöÑÊ®°ÂûãÊÄªÊï∞\n",
    "  remove_unused_columns=False, #Âà†Èô§Êú™‰ΩøÁî®ÂàóÔºåÂáèÂ∞ëÂÜÖÂ≠òÊ∂àËÄó\n",
    "  push_to_hub=False, #ÊòØÂê¶Â∞ÜÊ®°ÂûãÂèëÂ∏ÉÂà∞hfÁ§æÂå∫\n",
    "  report_to='tensorboard', #Êó•ÂøóÁöÑÊä•ÂëäÂú∞\n",
    "  load_best_model_at_end=True, #ÊòØÂê¶Ëá™Âä®Âä†ËΩΩÊúÄ‰ºòÊ®°Âûã\n",
    "  ignore_data_skip=True #‰ªéÊñ≠ÁÇπÁªßÁª≠ËÆ≠ÁªÉÊ®°Âûã\n",
    ")\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fc593f9-0a93-4585-9147-91fc7a3e4288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.8526\n",
      "  eval_f1                 =     0.8631\n",
      "  eval_loss               =     0.3775\n",
      "  eval_precision          =     0.9295\n",
      "  eval_recall             =     0.8056\n",
      "  eval_runtime            = 0:00:14.24\n",
      "  eval_samples_per_second =     21.895\n",
      "  eval_steps_per_second   =      2.737\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(dataset['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a603299-d1d5-43bc-8fba-b29b1d4a5e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
